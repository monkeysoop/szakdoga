{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plot_utils import plot_images, plot_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_calls_path = os.path.join(\"benchmarks\", \"performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_filenames = {}\n",
    "\n",
    "for dir_name in os.listdir(sdf_calls_path):\n",
    "    sub_dir_path = os.path.join(sdf_calls_path, dir_name)\n",
    "    if (os.path.isdir(sub_dir_path)):\n",
    "        benchmark_filenames[dir_name] = os.listdir(sub_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_images = {}\n",
    "time_meassurments = {}\n",
    "\n",
    "for method, filenames in benchmark_filenames.items():\n",
    "    folder_path = os.path.join(sdf_calls_path, method)\n",
    "\n",
    "    benchmark_images[method] = []\n",
    "    time_meassurments[method] = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        max_iteration_count = int((os.path.basename(file_path).split(\".\")[-2]).split(\"_\")[-1])\n",
    "        \n",
    "        if (filename.split(\".\")[-1] == \"png\"):\n",
    "            benchmark_image_bgr = cv2.imread(file_path)\n",
    "            benchmark_images[method].append((max_iteration_count, benchmark_image_bgr))\n",
    "        elif (filename.split(\".\")[-1] == \"txt\"):\n",
    "            time_meassurment_file = open(file_path, \"r+\")\n",
    "            mm = time_meassurment_file.readline().split(\" \")\n",
    "            time_meassurment_file.close()\n",
    "            time_meassurments[method].append((max_iteration_count, float(mm[0]), float(mm[1])))\n",
    "\n",
    "    benchmark_images[method].sort(key=lambda x: x[0])\n",
    "    time_meassurments[method].sort(key=lambda x: x[0])\n",
    "        \n",
    "benchmark_images = dict(sorted(benchmark_images.items(), key=lambda x: x[0])[::-1])\n",
    "time_meassurments = dict(sorted(time_meassurments.items(), key=lambda x: x[0])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_sdf_call_count_images_gray = {}\n",
    "avg_sdf_counts_per_pixel = {}\n",
    "max_iteration_counts = set()\n",
    "\n",
    "for (method, images) in benchmark_images.items():\n",
    "    benchmark_sdf_call_count_images_gray[method] = []\n",
    "    avg_sdf_counts_per_pixel[method] = []\n",
    "\n",
    "    for (max_iteration_count, image_bgr) in images:\n",
    "        image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        sdf_count_sum = int((cv2.sumElems(image_gray)[0] / 255.0) * max_iteration_count)\n",
    "        avg_sdf_count_per_pixel = sdf_count_sum / (image_gray.shape[0] * image_gray.shape[1])\n",
    "\n",
    "        benchmark_sdf_call_count_images_gray[method].append(image_gray)\n",
    "        avg_sdf_counts_per_pixel[method].append(round(avg_sdf_count_per_pixel, 3))\n",
    "        max_iteration_counts.add(max_iteration_count)\n",
    "\n",
    "max_iteration_counts = sorted(max_iteration_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_meassurments_chrono = {}\n",
    "time_meassurments_gl_querry = {}\n",
    "max_iteration_counts = set()\n",
    "\n",
    "for (method, meassurments) in time_meassurments.items():\n",
    "    time_meassurments_chrono[method] = []\n",
    "    time_meassurments_gl_querry[method] = []\n",
    "    \n",
    "    for (max_iteration_count, chrono, gl_querry) in meassurments:\n",
    "        time_meassurments_chrono[method].append(round(chrono, 3))\n",
    "        time_meassurments_gl_querry[method].append(round(gl_querry, 3))\n",
    "        max_iteration_counts.add(max_iteration_count)\n",
    "\n",
    "max_iteration_counts = sorted(max_iteration_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(\n",
    "    images=np.array(list(benchmark_sdf_call_count_images_gray.values())),\n",
    "    row_labels=[\" \".join(str(i).split(\"_\")) for i in list(benchmark_sdf_call_count_images_gray.keys())],\n",
    "    col_labels=[str(i) for i in list(max_iteration_counts)],\n",
    "    figsize=50,\n",
    "    x_label=\"Max iteration count\",\n",
    "    y_label=\"Tracing method\",\n",
    "    title=\"SDF call counts (divided by max iteration counts) of different tracing methods at different max iteration counts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(\n",
    "    matrix=np.array(list(avg_sdf_counts_per_pixel.values())),\n",
    "    row_labels=[\" \".join(str(i).split(\"_\")) for i in list(avg_sdf_counts_per_pixel.keys())],\n",
    "    col_labels=[str(i) for i in list(max_iteration_counts)],\n",
    "    meassurment_unit=\"\",\n",
    "    x_figsize=15,\n",
    "    y_figsize=6,\n",
    "    x_label=\"Max iteration count\",\n",
    "    y_label=\"Tracing method\",\n",
    "    title=\"Average sdf call count per pixel (not including: shadows, ao, normal calculations)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(\n",
    "    matrix=np.array(list(time_meassurments_chrono.values())),\n",
    "    row_labels=[\" \".join(str(i).split(\"_\")) for i in list(time_meassurments_chrono.keys())],\n",
    "    col_labels=[str(i) for i in list(max_iteration_counts)],\n",
    "    meassurment_unit=\"ms\",\n",
    "    x_figsize=15,\n",
    "    y_figsize=6,\n",
    "    x_label=\"Max iteration count\",\n",
    "    y_label=\"Tracing method\",\n",
    "    title=\"Averaged render times in ms (chrono)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(\n",
    "    matrix=np.array(list(time_meassurments_gl_querry.values())),\n",
    "    row_labels=[\" \".join(str(i).split(\"_\")) for i in list(time_meassurments_gl_querry.keys())],\n",
    "    col_labels=[str(i) for i in list(max_iteration_counts)],\n",
    "    meassurment_unit=\"ms\",\n",
    "    x_figsize=15,\n",
    "    y_figsize=6,\n",
    "    x_label=\"Max iteration count\",\n",
    "    y_label=\"Tracing method\",\n",
    "    title=\"Averaged render times in ms (gl querry)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
