{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plot_utils import plot_images, plot_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_calls_path = os.path.join(\"benchmarks\", \"performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_filenames = {}\n",
    "\n",
    "for dir_name in os.listdir(sdf_calls_path):\n",
    "    sub_dir_path = os.path.join(sdf_calls_path, dir_name)\n",
    "    if (os.path.isdir(sub_dir_path)):\n",
    "        benchmark_filenames[dir_name] = os.listdir(sub_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_images = {}\n",
    "time_meassurments = {}\n",
    "\n",
    "for method, filenames in benchmark_filenames.items():\n",
    "    folder_path = os.path.join(sdf_calls_path, method)\n",
    "\n",
    "    benchmark_images[method] = []\n",
    "    for filename in filenames:\n",
    "        if (filename.split(\".\")[-1] == \"png\"):\n",
    "            benchmark_file_path = os.path.join(folder_path, filename)\n",
    "            benchmark_image_bgr = cv2.imread(benchmark_file_path)\n",
    "            max_iteration_count = int((os.path.basename(benchmark_file_path).split(\".\")[-2]).split(\"_\")[-1])\n",
    "            benchmark_images[method].append((max_iteration_count, benchmark_image_bgr))\n",
    "        elif (filename.split(\".\")[-1] == \"txt\"):\n",
    "            time_meassurment_path = os.path.join(folder_path, filename)\n",
    "            time_meassurment_file = open(time_meassurment_path, \"r+\")\n",
    "            mm = time_meassurment_file.readline().split(\" \")\n",
    "            time_meassurment_file.close()\n",
    "            time_meassurments[method] = (float(mm[0]), float(mm[1]))\n",
    "\n",
    "    benchmark_images[method].sort(key=lambda x: x[0])\n",
    "        \n",
    "benchmark_images = dict(sorted(benchmark_images.items(), key=lambda x: x[0])[::-1])\n",
    "time_meassurments = dict(sorted(time_meassurments.items(), key=lambda x: x[0])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_sdf_call_count_images_gray = {}\n",
    "avg_sdf_counts_per_pixel = {}\n",
    "max_iter_counts = set()\n",
    "\n",
    "for y, (method, images) in enumerate(benchmark_images.items()):\n",
    "    benchmark_sdf_call_count_images_gray[method] = []\n",
    "    avg_sdf_counts_per_pixel[method] = []\n",
    "\n",
    "    for x, (max_iteration_count, image_bgr) in enumerate(images):\n",
    "        image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        sdf_count_sum = int((cv2.sumElems(image_gray)[0] / 255.0) * max_iteration_count)\n",
    "        avg_sdf_count_per_pixel = sdf_count_sum / (image_gray.shape[0] * image_gray.shape[1])\n",
    "\n",
    "        benchmark_sdf_call_count_images_gray[method].append(image_gray)\n",
    "        avg_sdf_counts_per_pixel[method].append(round(avg_sdf_count_per_pixel, 3))\n",
    "        max_iter_counts.add(max_iteration_count)\n",
    "\n",
    "max_iter_counts = sorted(max_iter_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(\n",
    "    np.array(list(benchmark_sdf_call_count_images_gray.values())),\n",
    "    [\" \".join(str(i).split(\"_\")) for i in list(benchmark_sdf_call_count_images_gray.keys())],\n",
    "    [str(i) for i in list(max_iter_counts)],\n",
    "    50,\n",
    "    35,\n",
    "    \"SDF call counts (divided by max iteration counts) of different tracing methods at different max iteration counts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(\n",
    "    np.array(list(avg_sdf_counts_per_pixel.values())),\n",
    "    [\" \".join(str(i).split(\"_\")) for i in list(avg_sdf_counts_per_pixel.keys())],\n",
    "    [str(i) for i in list(max_iter_counts)],\n",
    "    \"\",\n",
    "    10,\n",
    "    6,\n",
    "    \"Max iteration count\",\n",
    "    \"Tracing method\",\n",
    "    \"Average sdf call count per pixel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(\n",
    "    np.array(list(time_meassurments.values())),\n",
    "    [\" \".join(str(i).split(\"_\")) for i in list(time_meassurments.keys())],\n",
    "    [\"chrono\", \"gl querry\"],\n",
    "    \"ms\",\n",
    "    5,\n",
    "    6,\n",
    "    \"Meassurment method\",\n",
    "    \"Tracing method\",\n",
    "    \"Averaged render times in ms\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
